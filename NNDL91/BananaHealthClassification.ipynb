{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKqQBS00NhC9"
      },
      "source": [
        "# Banana health classification\n",
        "\n",
        "An ML model for image classification of healthy and diseased banana leafs. Uses binary classification to determine health state, based on two powerful ML models.\n",
        "\n",
        "This notebook is part of the \"Neuronale Netze & Deep Learning\" course at [Digital Business University of Applied Sciences (DBU)](https://dbuas.de/). See the attached report \"Analyse der Gesundheit von Bananenpflanzen durch Bild-Klassifizierung mittels maschineller Lernverfahren\" (german) for more insights.\n",
        "\n",
        "**‚ö†Ô∏è Please read the additional setup instructions in the attached project report (section \"A. Systemanforderungen\") before running this notebook.**\n",
        "\n",
        "Author: [Elias H√§u√üler](https://haeussler.dev) &middot; Developed in July 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObCKmHbd_OB0"
      },
      "source": [
        "## 1.) Preparation\n",
        "\n",
        "In the following steps, the **Python environment** is prepared and all required modules are imported. This includes configuration of **Kaggle authentication** which requires you to type in your username and personal Kaggle API key. You can find both values in a `kaggle.json` file which can be downloaded from your Kaggle [account settings](https://www.kaggle.com/settings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNNUSY0RBo7E"
      },
      "source": [
        "### 1.1.) Import modules and install libraries\n",
        "\n",
        "This section installs and imports all relevant Python modules and external libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8zF1epHebGg"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "  \"kaggle==1.6.17\" \\\n",
        "  \"keras-tuner==1.4.7\" \\\n",
        "  \"matplotlib==3.7.1\" \\\n",
        "  \"numpy==1.26.4\" \\\n",
        "  \"pandas==2.1.4\" \\\n",
        "  \"scikit-learn==1.3.2\" \\\n",
        "  \"seaborn==0.13.1\" \\\n",
        "  \"tensorflow==2.15.0\" \\\n",
        "  \"tensorflow-hub==0.16.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avpXXf3bihMs"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import math\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from platform import python_version\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras_tuner as kt\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dNgoo6ri0q7"
      },
      "outputs": [],
      "source": [
        "print(\"Python version:\", python_version())\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available üèéÔ∏è\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE üöú\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltPnGc0SBMdw"
      },
      "source": [
        "### 1.2.) Authentication at Kaggle\n",
        "\n",
        "Please provide your authentication data for Kaggle. You can lookup your username and API key in a `kaggle.json` file which can be downloaded from your Kaggle account settings. Read more at the [official documentation](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md#api-credentials)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLPtHhQ3ahNP"
      },
      "outputs": [],
      "source": [
        "kaggle_user = input('Please enter your Kaggle username: ')\n",
        "kaggle_key = input('Please enter your Kaggle API token: ')\n",
        "\n",
        "%env KAGGLE_USERNAME=$kaggle_user\n",
        "%env KAGGLE_KEY=$kaggle_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Zywqlz__cE"
      },
      "source": [
        "## 2.) Data colletion\n",
        "\n",
        "In this section, we **prepare the dataset** used to train and test our model. For this project, the [`BananaLSD`](https://www.kaggle.com/datasets/shifatearman/bananalsd) dataset from Kaggle is used. The dataset provides an original set as well as an augmented set with preprocessed images using data augmentation. However, we will use the **original set** and perform data augmentation on our own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDVzxqspB9Zm"
      },
      "source": [
        "### 2.1.) Download and initialize dataset\n",
        "\n",
        "This section downloads the dataset from Kaggle. Once downloaded, the dataset is split into training and validation datasets. The used validation split is 30%. We enable batching with 32 data points per batch and define our image dimensions (224x224 pixels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-GV2F5dmqip"
      },
      "outputs": [],
      "source": [
        "training_dir = os.path.join(os.getcwd(), 'BananaHealthClassification')\n",
        "dataset_dir = os.path.join(training_dir, 'dataset')\n",
        "data_dir = os.path.join(dataset_dir, 'BananaLSD', 'OriginalSet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kNmcF0CbZ_K"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(data_dir):\n",
        "  !kaggle datasets download -d shifatearman/bananalsd -p $dataset_dir --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnRq7cHZqrv8"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "batch_size = 32\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "image_shape = (image_height, image_width, 3)\n",
        "validation_split = 0.3\n",
        "\n",
        "(train_ds, val_ds) = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  seed = seed,\n",
        "  validation_split = validation_split,\n",
        "  subset = 'both',\n",
        "  image_size = (image_height, image_width),\n",
        "  batch_size = batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvfmyAkgvJA"
      },
      "source": [
        "### 2.2.) Convert multiclass to binary class data\n",
        "\n",
        "Since we're just interested in the leaves being healthy or having a disease, we convert the labels of each data point to their corresponding health state:\n",
        "\n",
        "* `0` = leaves are healthy\n",
        "* `1` = leaves have disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykAhW5b6tTpD"
      },
      "outputs": [],
      "source": [
        "initial_class_names = np.array(train_ds.class_names)\n",
        "initial_healthy_class_index = np.where(initial_class_names == 'healthy')[0][0]\n",
        "initial_disease_class_indexes = np.where(initial_class_names != 'healthy')[0]\n",
        "\n",
        "print(f'Initial class names: {initial_class_names}')\n",
        "print(f'Initial \"healthy\" class has index {initial_healthy_class_index}.')\n",
        "print(f'Initial \"disease\" classes have indexes {initial_disease_class_indexes}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLYidnCohyDY"
      },
      "outputs": [],
      "source": [
        "healthy_class_index = 0\n",
        "disease_class_index = 1\n",
        "class_names = np.array([healthy_class_index, disease_class_index])\n",
        "class_labels = {\n",
        "  healthy_class_index: 'Healthy',\n",
        "  disease_class_index: 'Disease',\n",
        "}\n",
        "\n",
        "print(f'Modified class names: {class_names}')\n",
        "print(f'Modified \"healthy\" class has index {healthy_class_index}.')\n",
        "print(f'Modified \"disease\" class has index {disease_class_index}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dciVJi3NdbwF"
      },
      "outputs": [],
      "source": [
        "def convert_labels(images, labels):\n",
        "  \"\"\"\n",
        "  Convert image labels from multiclass to binary class state (healthy/disease).\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert healthy class\n",
        "  converted_labels = tf.where(\n",
        "    labels == initial_healthy_class_index,\n",
        "    tf.cast(healthy_class_index, tf.int32),\n",
        "    labels,\n",
        "  )\n",
        "\n",
        "  # Convert disease classes\n",
        "  for initial_disease_class_index in initial_disease_class_indexes:\n",
        "    converted_labels = tf.where(\n",
        "      labels == initial_disease_class_index,\n",
        "      tf.cast(disease_class_index, tf.int32),\n",
        "      converted_labels,\n",
        "    )\n",
        "\n",
        "  # Convert to float values for the F1 score to work properly\n",
        "  converted_labels = tf.cast(converted_labels, tf.float32)\n",
        "\n",
        "  return images, converted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8kuFFPfelv-"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(convert_labels)\n",
        "val_ds = val_ds.map(convert_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYA1OYdPKPP"
      },
      "source": [
        "### 2.3.) Lookup data samples\n",
        "\n",
        "Once initialized, we want to take a quick look at our image dataset. For this, we render some samples from the first image batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZFTgPsUAsc3"
      },
      "outputs": [],
      "source": [
        "def normalize_label(label):\n",
        "  \"\"\"\n",
        "  Normalize label value from float to integer.\n",
        "  \"\"\"\n",
        "\n",
        "  return label.numpy().astype('uint8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr1vePAuGr6I"
      },
      "outputs": [],
      "source": [
        "def label_to_class(label):\n",
        "  \"\"\"\n",
        "  Convert given label value to speaking label.\n",
        "  \"\"\"\n",
        "\n",
        "  return class_labels[normalize_label(label)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0HzF8Fx6WMi"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images_from_dataset(image_batch, rows = None, title = 'Samples from image batch'):\n",
        "  \"\"\"\n",
        "  Render sample images from dataset using subplots.\n",
        "  \"\"\"\n",
        "\n",
        "  cols = 3\n",
        "\n",
        "  for images, labels in image_batch:\n",
        "    images_len = len(images)\n",
        "    images_range = range(images_len if rows == None else rows * cols)\n",
        "\n",
        "    if rows == None:\n",
        "      rows = math.ceil(images_len / cols)\n",
        "\n",
        "    plt.figure(figsize = (cols * 4, rows * 4))\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    for i in images_range:\n",
        "      if images_len > i:\n",
        "        axes = plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype('uint8'))\n",
        "        plt.title(f'{normalize_label(labels[i])} ({label_to_class(labels[i])})')\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZfjyhkW6XTo"
      },
      "outputs": [],
      "source": [
        "plot_sample_images_from_dataset(\n",
        "  train_ds.take(1),\n",
        "  title = 'Training images (sample batch)',\n",
        "  rows = 3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ArS4OEPjp7"
      },
      "source": [
        "## 3.) Modelling\n",
        "\n",
        "In order to build our model, we first need to make sure that our data is properly **balanced**. This is an essential step to assure that our model performs well on unknown data. In subsequent steps, the model is defined by all its components, for example by two base models using **Transfer Learning** and combined methods to address **regularization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkT3EbCFRmp6"
      },
      "source": [
        "### 3.1.) Improve training performance\n",
        "\n",
        "Before starting our modelling process, we modify our training and validation datasets to perform better during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnaNjljtR2UB"
      },
      "outputs": [],
      "source": [
        "def cache_datasets(train_ds, val_ds):\n",
        "  \"\"\"\n",
        "  Cache and prefetch training and validation datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  train_ds = train_ds.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "  val_ds = val_ds.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "\n",
        "  return train_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuKNslRo0-w0"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = cache_datasets(train_ds, val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-6_Yby9QqwT"
      },
      "source": [
        "### 3.2.) Check class imbalance\n",
        "\n",
        "We need to make sure that our data is properly balanced. Otherwise, the model may perform better on the overrepresented classes. That's why we evaluate our training data in terms of class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg3IcRss4h9c"
      },
      "outputs": [],
      "source": [
        "def count_datapoints(dataset):\n",
        "  \"\"\"\n",
        "  Count datapoints of each class in the dataset.\n",
        "\n",
        "  Returns a data frame with classes and number of datapoints (samples) per class.\n",
        "  \"\"\"\n",
        "\n",
        "  dict_counts = defaultdict(int)\n",
        "\n",
        "  for _, labels_batch in dataset:\n",
        "    for label in labels_batch:\n",
        "      class_name = label_to_class(label)\n",
        "      dict_counts[class_name] += 1\n",
        "\n",
        "  dict_counts_df = pd.DataFrame(\n",
        "    dict_counts.items(),\n",
        "    columns = ['class', 'samples'],\n",
        "  )\n",
        "\n",
        "  return dict_counts_df.sort_values(by = 'class', ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwR8PMOR5sFM"
      },
      "outputs": [],
      "source": [
        "def plot_datapoints(data_frame, column, title):\n",
        "  \"\"\"\n",
        "  Render bar chart and pie chart of given data and selected column.\n",
        "  \"\"\"\n",
        "\n",
        "  healthy_count = data_frame[column][datapoints_count['class'] == 'Healthy'].sum()\n",
        "  disease_count = data_frame[column][datapoints_count['class'] == 'Disease'].sum()\n",
        "\n",
        "  data = [healthy_count, disease_count]\n",
        "  labels = ['Healthy', 'Disease']\n",
        "  colors = ['lightgreen', 'coral']\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(\n",
        "    nrows = 1,\n",
        "    ncols = 2,\n",
        "    width_ratios = [0.3, 0.7],\n",
        "    figsize = (12, 6),\n",
        "  )\n",
        "  fig.suptitle(title)\n",
        "\n",
        "  # Render bar chart\n",
        "  ax[0].bar(\n",
        "    labels,\n",
        "    data,\n",
        "    color = colors,\n",
        "  )\n",
        "  ax[0].spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    ax[0].text(i, data[i] / 2, str(round(data[i], 2)), ha = 'center')\n",
        "\n",
        "  # Render pie chart\n",
        "  wedges, texts, autotexts = ax[1].pie(\n",
        "    data,\n",
        "    colors = colors,\n",
        "    autopct = lambda x: str(round(x, 2)) + '%',\n",
        "    explode = (0.1, 0),\n",
        "    textprops = dict(color = 'white'),\n",
        "  )\n",
        "  ax[1].legend(\n",
        "    wedges,\n",
        "    labels,\n",
        "    loc = 'upper right',\n",
        "    bbox_to_anchor = (1, 0, 0.5, 1),\n",
        "  )\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PPBMv955tb4"
      },
      "outputs": [],
      "source": [
        "datapoints_count = count_datapoints(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd8sh6bb5t8x"
      },
      "outputs": [],
      "source": [
        "plot_datapoints(datapoints_count, 'samples', 'Samples per class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DgDAux6CtCr"
      },
      "source": [
        "### 3.3.) Handle class imbalance\n",
        "\n",
        "Our dataset contains an imbalanced set of data per class. That's why we use several techniques to handle class imbalance in our dataset. The following sections describe all techniques in a meaningful way and visualize how each technique steps in the process of resolving our class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GF5n4OTawxR"
      },
      "source": [
        "#### 3.3.1.) Class Weights\n",
        "\n",
        "This step calculates the weight of each class and visualizes the class imbalance based on the calculated class weights. This way, we can again recognize the class imbalance in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQmutZ5eh_8k"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights():\n",
        "  \"\"\"\n",
        "  Calculate class weights in training dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  labels = np.concatenate([y for x, y in train_ds], axis = 0)\n",
        "  class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes = np.unique(labels),\n",
        "    y = labels,\n",
        "  )\n",
        "\n",
        "  return dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPJmVbslqWFf"
      },
      "outputs": [],
      "source": [
        "class_weights = calculate_class_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Np3id_fazsF"
      },
      "outputs": [],
      "source": [
        "datapoints_count.insert(0, 'weight', class_weights.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l1J_pYcfh0P"
      },
      "outputs": [],
      "source": [
        "plot_datapoints(datapoints_count, 'weight', 'Weight per class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfJZ9nskNE9N"
      },
      "source": [
        "#### 3.3.2.) Data Augmentation\n",
        "\n",
        "In this step, we define data augmentation for our dataset. This allows us to better train our model and, in addition, handle class imbalance. An example of data augmentation is visualized for a sample batch.\n",
        "\n",
        "After that, we use data augmentation to perform oversampling. We generate 1000 images per class using data augmentation, while keeping the existing data. As a result, we should see two balanced classes in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-trUka-qOM-u"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\n",
        "    'horizontal_and_vertical',\n",
        "    input_shape = image_shape,\n",
        "  ),\n",
        "  tf.keras.layers.RandomRotation(0.5),\n",
        "  tf.keras.layers.RandomZoom(0.5),\n",
        "  tf.keras.layers.RandomBrightness(0.2),\n",
        "  tf.keras.layers.RandomContrast(0.2),\n",
        "  tf.keras.layers.RandomWidth(0.2),\n",
        "  tf.keras.layers.RandomHeight(0.2),\n",
        "  tf.keras.layers.GaussianNoise(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhea5xHTpWpX"
      },
      "outputs": [],
      "source": [
        "plot_sample_images_from_dataset(\n",
        "  train_ds.take(1).map(lambda x, y: (data_augmentation(x, training = True), y)),\n",
        "  title = 'Augmented training images (sample batch)',\n",
        "  rows = 3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmiv5vENuvBM"
      },
      "outputs": [],
      "source": [
        "def split_images_by_class(dataset):\n",
        "  \"\"\"\n",
        "  Split images of dataset into healthy and disease images.\n",
        "  \"\"\"\n",
        "\n",
        "  healthy_images = []\n",
        "  disease_images = []\n",
        "\n",
        "  for images, labels in dataset:\n",
        "    for i in range(len(labels)):\n",
        "      if labels[i] == healthy_class_index:\n",
        "        healthy_images.append(images[i])\n",
        "      else:\n",
        "        disease_images.append(images[i])\n",
        "\n",
        "  return np.array(healthy_images), np.array(disease_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je8IEYTuvJrx"
      },
      "outputs": [],
      "source": [
        "def copy_and_augment_images(images, target_class_count = 1000):\n",
        "  \"\"\"\n",
        "  Perform data augmentation on given images until target class count is reached.\n",
        "  \"\"\"\n",
        "\n",
        "  augmented_images = list(images.copy())\n",
        "\n",
        "  while len(augmented_images) < target_class_count:\n",
        "    for x in images:\n",
        "      # Normalize image shape for expected input in augmentation layer\n",
        "      augmented_image = tf.squeeze(data_augmentation(tf.expand_dims(x, 0)), 0)\n",
        "      augmented_image = tf.image.resize(augmented_image, [image_height, image_width])\n",
        "      augmented_images.append(augmented_image)\n",
        "\n",
        "      if len(augmented_images) >= target_class_count:\n",
        "        break\n",
        "\n",
        "  return np.array(augmented_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTwr582Yh2c4"
      },
      "outputs": [],
      "source": [
        "# Perform data augmentation on full dataset\n",
        "full_dataset = train_ds.concatenate(val_ds)\n",
        "healthy_images, disease_images = split_images_by_class(full_dataset)\n",
        "healthy_augmented_images = copy_and_augment_images(healthy_images)\n",
        "disease_augmented_images = copy_and_augment_images(disease_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZ5TXDE3Mhu"
      },
      "outputs": [],
      "source": [
        "# Prepare augmented images and labels\n",
        "augmented_images = np.concatenate((healthy_augmented_images, disease_augmented_images))\n",
        "augmented_labels = np.array(\n",
        "  # Cast to float is relevant for the F1 score to work properly\n",
        "  [tf.cast(healthy_class_index, tf.float32)] * len(healthy_augmented_images) +\n",
        "  [tf.cast(disease_class_index, tf.float32)] * len(disease_augmented_images),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6xdnGuV1niQ"
      },
      "outputs": [],
      "source": [
        "# Split augmented training and validation data\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "  augmented_images,\n",
        "  augmented_labels,\n",
        "  test_size = validation_split,\n",
        "  random_state = seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWT2Lfbz3OEW"
      },
      "outputs": [],
      "source": [
        "# Convert augmented data back to TF datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(batch_size)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4MD3ms18CiR"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = cache_datasets(train_ds, val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBP6Asxo2QiT"
      },
      "source": [
        "### 3.4.) Validate class balance\n",
        "\n",
        "It's time to validate whether we properly handled class imbalance. In this step, we visualize the samples and weights per class again. We can see that our classes are now properly balanced, which allows us to continue our modeling process effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63wewq5e2UwQ"
      },
      "outputs": [],
      "source": [
        "datapoints_count = count_datapoints(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNYF9VSK2Zl6"
      },
      "outputs": [],
      "source": [
        "plot_datapoints(datapoints_count, 'samples', 'Samples per class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aXiu8Mx2dUt"
      },
      "outputs": [],
      "source": [
        "class_weights = calculate_class_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBKXGwyN2gxi"
      },
      "outputs": [],
      "source": [
        "datapoints_count.insert(0, 'weight', class_weights.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gY8Lms-2lXR"
      },
      "outputs": [],
      "source": [
        "plot_datapoints(datapoints_count, 'weight', 'Weight per class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAS1zCbW4Q_5"
      },
      "source": [
        "### 3.5.) Build model\n",
        "\n",
        "Now that our dataset is properly initialized and balanced, we can start building our model. We use several techniques to improve the training process and make the model perform as good as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsH3jo0KhMZT"
      },
      "source": [
        "#### 3.5.1.) Select base models (Transfer Learning)\n",
        "\n",
        "In the first step, we select two pretrained models as our base models. This technique is called **Transfer Learning** and allows our model to make better predictions since we can rely on a lot of previous trainings and a huge training dataset.\n",
        "\n",
        "We use the following two base models:\n",
        "\n",
        "1. The [**EfficientNet V2**](https://www.kaggle.com/models/google/efficientnet-v2) model in the variation *Efficientnetv2 B0 (21K)* is trained on the *ImageNet-21K* dataset and specifically on Google's *flower images* dataset, which targets a broad variety of our custom training dataset. We use the model's variation that provides additional feature vectors in order to get more power out of the CNN behind it.\n",
        "2. The more specific [**rishitdagli/plant-disease**](https://www.kaggle.com/models/rishitdagli/plant-disease) model is trained on a various set of plant diseases. It allows us to re-use similar training data for detections of banana leaf diseases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwvGtj8m9bdi"
      },
      "outputs": [],
      "source": [
        "base_model_1 = hub.KerasLayer(\n",
        "  'https://www.kaggle.com/models/google/efficientnet-v2/frameworks/TensorFlow2/variations/imagenet21k-ft1k-b0-feature-vector/versions/1',\n",
        "  trainable = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGFvdcy_XZql"
      },
      "outputs": [],
      "source": [
        "base_model_2 = hub.KerasLayer(\n",
        "  'https://www.kaggle.com/models/rishitdagli/plant-disease/TensorFlow2/plant-disease/1',\n",
        "  trainable = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qii1WJ55h42m"
      },
      "source": [
        "#### 3.5.2.) Define layers\n",
        "\n",
        "The base models as part of our model are defined as additional concatenated input layer. Next to it, we add some more layers and use them to define our model:\n",
        "\n",
        "1. We include the previously defined **data augmentation** layer.\n",
        "2. We add a **rescaling layer** to assure that input data is properly converted to the expected Tensor format.\n",
        "3. In order to address *regularization*, we also add a **dropout layer**.\n",
        "4. We now include the **base models** as additional input layer.\n",
        "5. The final output layers consist of two **dense layers**. The first one activates some neurons to detect patterns. The second (and last) one describes the final output layer containing the model's prediction. In addition, we define an L2 regularizer as an additional regularization measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2kQzVuHnbW4"
      },
      "outputs": [],
      "source": [
        "def define_layers(dense_units = 480, dropout_rate = 0.5, l2_factor = 0.001):\n",
        "  \"\"\"\n",
        "  Define all relevant layers for our resulting model using the given parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  # Input layers\n",
        "  input_layer = tf.keras.Input(shape = image_shape)\n",
        "  inputs = data_augmentation(input_layer)\n",
        "  inputs = tf.keras.layers.Rescaling(1./255)(inputs)\n",
        "  inputs = tf.keras.layers.Dropout(dropout_rate)(inputs)\n",
        "\n",
        "  # Output layers\n",
        "  model_1_output = base_model_1(inputs)\n",
        "  model_2_output = base_model_2(inputs)\n",
        "  outputs = tf.keras.layers.concatenate([\n",
        "    model_1_output,\n",
        "    model_2_output,\n",
        "  ])\n",
        "  outputs = tf.keras.layers.Dense(\n",
        "    units = dense_units,\n",
        "    activation = 'relu',\n",
        "  )(outputs)\n",
        "  outputs = tf.keras.layers.Dense(\n",
        "    # We use an explicit dense layer of 1 for our binary classification problem\n",
        "    units = 1,\n",
        "    activation = 'sigmoid',\n",
        "    kernel_regularizer = tf.keras.regularizers.l2(l2_factor),\n",
        "  )(outputs)\n",
        "\n",
        "  return input_layer, outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vEpjrRAjE0v"
      },
      "source": [
        "#### 3.5.3.) Compile model\n",
        "\n",
        "Once the model is described with our supported layers, we can now define its compilation process. We use *Adam* as **compiler** for our model since it performs very well as generic compiler. In order to calculate the **crossentropy loss** between labels and predictions, we use *binary crossentropy*. The relevant **metrics** during fitting of our model are the following:\n",
        "\n",
        "* We use the **accuracy** metric to measure correct predictions during fitting.\n",
        "* We use the **F1 score** to measure performance of our model, specifically in context of image classification.\n",
        "* We use the **precision** metric to validate a low rate of false positives.\n",
        "* Last but not least, we use the **recall** metric to validate a low rate of false negatives. This is an essential metric for our model, because we must avoid predictions of healthy leaves while they're actually having a disease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hy-h9SFpdry"
      },
      "outputs": [],
      "source": [
        "def create_metrics():\n",
        "  \"\"\"\n",
        "  Define all evaluation metrics to train our model.\n",
        "  \"\"\"\n",
        "\n",
        "  return [\n",
        "    tf.keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
        "    tf.keras.metrics.F1Score(threshold = 0.5, name = 'f1_score'),\n",
        "    tf.keras.metrics.Precision(name = 'precision'),\n",
        "    tf.keras.metrics.Recall(name = 'recall'),\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwvNeoU--2aM"
      },
      "outputs": [],
      "source": [
        "def compile_model(model, optimizer = 'adam', learning_rate = 0.001):\n",
        "  \"\"\"\n",
        "  Compile model with given learning rate for given optimizer.\n",
        "\n",
        "  Creates several metrics and uses them together with the given optimizer and\n",
        "  a binary crossentropy loss to compile the given model.\n",
        "  \"\"\"\n",
        "\n",
        "  if optimizer == 'rmsprop':\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "  elif optimizer == 'sgd':\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
        "  else:\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics = create_metrics(),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22iSKUthkDH8"
      },
      "source": [
        "#### 3.5.4.) Define callbacks\n",
        "\n",
        "We use **Early Stopping** as relevant callback in the fitting process of our model. It is based on the validation loss and serves as an additional measure for regularization and to avoid overfitting of our model. We choose to stop in case training won't be improved after 10 epochs (using the `patience` argument)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYICp0OkCQko"
      },
      "outputs": [],
      "source": [
        "def create_early_stopping_callback():\n",
        "  return tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    patience = 10,\n",
        "    restore_best_weights = True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2Ai4vJ7qTN"
      },
      "source": [
        "#### 3.5.5.) Free up RAM\n",
        "\n",
        "For the following resource-intensive operations, we remove some variables to free up RAM. This stabilizes further operations and avoids session timeouts due to full RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L52OTzWU7pZy"
      },
      "outputs": [],
      "source": [
        "%reset_selective -f ^initial_class_names$\n",
        "%reset_selective -f ^initial_healthy_class_index$\n",
        "%reset_selective -f ^initial_disease_class_indexes$\n",
        "%reset_selective -f ^full_dataset$\n",
        "%reset_selective -f ^healthy_images$\n",
        "%reset_selective -f ^disease_images$\n",
        "%reset_selective -f ^healthy_augmented_images$\n",
        "%reset_selective -f ^disease_augmented_images$\n",
        "%reset_selective -f ^augmented_images$\n",
        "%reset_selective -f ^augmented_labels$\n",
        "%reset_selective -f ^train_images$\n",
        "%reset_selective -f ^val_images$\n",
        "%reset_selective -f ^train_labels$\n",
        "%reset_selective -f ^val_labels$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zJ9aXgTu6mF"
      },
      "source": [
        "## 4.) Training\n",
        "\n",
        "Now that our model is properly defined, we can start fitting and thus training of our model. We use our training dataset as input data and define our validation dataset to evaluate loss and other metrics for each epoch. We use a maxmimum of **30 epochs** for fitting of our model. Since _Early Stopping_ is enabled, the actual number of performed epochs may be lower.\n",
        "\n",
        "The training is split in two steps:\n",
        "\n",
        "1. **Manual training**: We choose four different hyperparameters in four iterations to test our model.\n",
        "2. **Automatic training**: We use *Keras Tuner* to automatically find the best hyperparameters for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pd39bPtlmK3"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, epochs = 30):\n",
        "  \"\"\"\n",
        "  Fit given model using our training and validation datasets.\n",
        "\n",
        "  Performs fitting for given number of epochs. Includes previously calculated\n",
        "  class weights and Early Stopping callback to avoid overfitting.\n",
        "  \"\"\"\n",
        "\n",
        "  return model.fit(\n",
        "    train_ds,\n",
        "    epochs = epochs,\n",
        "    validation_data = val_ds,\n",
        "    class_weight = class_weights,\n",
        "    callbacks = [create_early_stopping_callback()],\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_train(dense_units, dropout_rate, l2_factor, optimizer, learning_rate):\n",
        "  \"\"\"\n",
        "  Build and train our model using the given hyperparameters.\n",
        "\n",
        "  Returns the trained model and the training history for further evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  input_layer, outputs = define_layers(\n",
        "    dense_units = dense_units,\n",
        "    dropout_rate = dropout_rate,\n",
        "    l2_factor = l2_factor,\n",
        "  )\n",
        "  model = tf.keras.Model(inputs = input_layer, outputs = outputs)\n",
        "  compile_model(\n",
        "    model = model,\n",
        "    optimizer = optimizer,\n",
        "    learning_rate = learning_rate,\n",
        "  )\n",
        "  history = fit_model(model)\n",
        "\n",
        "  return model, history.history"
      ],
      "metadata": {
        "id": "Qwc4c6qfIFSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zez5GZzYpn7F"
      },
      "outputs": [],
      "source": [
        "def read_metric(metric: str, history: dict):\n",
        "  \"\"\"\n",
        "  Return dict with training and validation score of given metric.\n",
        "  \"\"\"\n",
        "\n",
        "  return {\n",
        "    'train': history[metric],\n",
        "    'val': history[f'val_{metric}'],\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR2aTkiHDQIE"
      },
      "outputs": [],
      "source": [
        "def read_all_metrics(history: dict):\n",
        "  \"\"\"\n",
        "  Read and return dict with all metrics of given training history.\n",
        "  \"\"\"\n",
        "\n",
        "  accuracy = read_metric('accuracy', history)\n",
        "  f1_score = read_metric('f1_score', history)\n",
        "  precision = read_metric('precision', history)\n",
        "  recall = read_metric('recall', history)\n",
        "  loss = read_metric('loss', history)\n",
        "\n",
        "  # Flatten list values in f1_score\n",
        "  f1_score['train'] = [x[0] for x in f1_score['train']]\n",
        "  f1_score['val'] = [x[0] for x in f1_score['val']]\n",
        "\n",
        "  return (accuracy, f1_score, precision, recall, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UokzdaVYfKxM"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(metrics: dict, show_title = True):\n",
        "  \"\"\"\n",
        "  Plot given metrics.\n",
        "  \"\"\"\n",
        "\n",
        "  label_mapping = {\n",
        "    'train': 'Training',\n",
        "    'val': 'Validation',\n",
        "  }\n",
        "  linestyle_mapping = {\n",
        "    'train': '-',\n",
        "    'val': '--',\n",
        "  }\n",
        "\n",
        "  metrics_len = len(metrics)\n",
        "  ncols = 2\n",
        "  nrows = math.ceil(metrics_len / ncols)\n",
        "\n",
        "  fig, ax = plt.subplots(\n",
        "    nrows = nrows,\n",
        "    ncols = ncols,\n",
        "    figsize = (12, nrows * 4),\n",
        "    squeeze = False,\n",
        "  )\n",
        "\n",
        "  # Plot each metric\n",
        "  for i, label in enumerate(metrics):\n",
        "    metric = metrics[label]\n",
        "    row = math.floor(i / ncols)\n",
        "    col = i % ncols\n",
        "\n",
        "    # Add plots\n",
        "    for y in metric:\n",
        "      values = metric[y]\n",
        "      y_label = f'{label_mapping[y] or y} {label}'\n",
        "      line_style = linestyle_mapping[y] or '-'\n",
        "      epochs = range(1, len(values) + 1)\n",
        "\n",
        "      ax[row, col].plot(epochs, values, label = y_label, linestyle = line_style)\n",
        "\n",
        "    # Configure axes\n",
        "    ax[row, col].set_title(label)\n",
        "    ax[row, col].set_xlim(left = 1)\n",
        "    ax[row, col].legend()\n",
        "    ax[row, col].grid(axis = 'y')\n",
        "\n",
        "  # Remove remaining axes\n",
        "  if i + 1 < nrows * ncols:\n",
        "    for j in range(i + 1, nrows * ncols):\n",
        "      row = math.floor(j / ncols)\n",
        "      col = j % ncols\n",
        "\n",
        "      fig.delaxes(ax[row, col])\n",
        "\n",
        "  # Set labels and title\n",
        "  if show_title:\n",
        "    if metrics_len > 1:\n",
        "      fig.suptitle('Training and Validation metrics')\n",
        "    else:\n",
        "      fig.suptitle(f'Training and Validation {label}')\n",
        "\n",
        "  # Render figure\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTVU0wHFndmT"
      },
      "outputs": [],
      "source": [
        "def plot_metric(metric: list, label: str):\n",
        "  \"\"\"\n",
        "  Plot a single metric.\n",
        "  \"\"\"\n",
        "\n",
        "  plot_metrics({label: metric})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg1_MdkXU6jl"
      },
      "outputs": [],
      "source": [
        "def plot_all_metrics(accuracy, f1_score, precision, recall, loss):\n",
        "  \"\"\"\n",
        "  Plot all given metrics.\n",
        "  \"\"\"\n",
        "\n",
        "  plot_metrics({\n",
        "    'Accuracy': accuracy,\n",
        "    'F1 score': f1_score,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'Loss': loss,\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(history: dict):\n",
        "  \"\"\"\n",
        "  Reads metrics from training history and plots each metric history.\n",
        "  \"\"\"\n",
        "\n",
        "  accuracy, f1_score, precision, recall, loss = read_all_metrics(history)\n",
        "  plot_metrics(\n",
        "    {\n",
        "      'Accuracy': accuracy,\n",
        "      'F1 score': f1_score,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "    },\n",
        "    show_title = False,\n",
        "  )"
      ],
      "metadata": {
        "id": "_5WeFs9r1neY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL7u2xVdu4_7"
      },
      "outputs": [],
      "source": [
        "def perform_predictions(model):\n",
        "  \"\"\"\n",
        "  Run predictions on given model with random batch of validation dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  test_images, test_labels = next(iter(val_ds.shuffle(batch_size)))\n",
        "\n",
        "  predictions = model.predict(test_images)\n",
        "  predicted_labels = np.round(predictions).astype(int).flatten()\n",
        "\n",
        "  false_images = test_images[test_labels != predicted_labels]\n",
        "  false_labels = predicted_labels[test_labels != predicted_labels]\n",
        "  false_predictions_len = len(false_images)\n",
        "\n",
        "  if false_predictions_len == 0:\n",
        "    print('There were no false predictions.')\n",
        "  elif false_predictions_len == 1:\n",
        "    print('There was one false prediction.')\n",
        "  else:\n",
        "    print(f'There were {false_predictions_len} false predictions.')\n",
        "\n",
        "  return (test_labels, predicted_labels, false_images, false_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXxs79JexDvs"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(cm):\n",
        "  \"\"\"\n",
        "  Render given confusion matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  labels = class_labels.values()\n",
        "\n",
        "  plt.figure(figsize = (10, 8))\n",
        "  plt.suptitle('Confusion matrix')\n",
        "\n",
        "  sns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt = 'g')\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Label')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7WIqjHC6DkS"
      },
      "outputs": [],
      "source": [
        "def plot_false_predictions(false_images, false_labels):\n",
        "  \"\"\"\n",
        "  Plot false model predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  false_images_ds = tf.data.Dataset.from_tensor_slices(false_images).batch(batch_size)\n",
        "  false_labels_ds = tf.data.Dataset.from_tensor_slices(false_labels).batch(batch_size)\n",
        "  false_predictions_ds = tf.data.Dataset.zip((false_images_ds, false_labels_ds))\n",
        "\n",
        "  plot_sample_images_from_dataset(\n",
        "    false_predictions_ds,\n",
        "    title = 'False predictions',\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmkmWrFwxmke"
      },
      "outputs": [],
      "source": [
        "def predict_and_evaluate(model):\n",
        "  \"\"\"\n",
        "  Perform predictions on given model and visuzalize results.\n",
        "  \"\"\"\n",
        "\n",
        "  test_labels, predicted_labels, false_images, false_labels = perform_predictions(model)\n",
        "  cm = confusion_matrix(test_labels, predicted_labels)\n",
        "  show_confusion_matrix(cm)\n",
        "  plot_false_predictions(false_images, false_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.) Manual training\n",
        "\n",
        "The manual training is split in four iterations. In each iteration, our model is trained with different hyperparameters. After fitting, the metrics are visualized and the trained model is used to make predictions for the validation dataset."
      ],
      "metadata": {
        "id": "jBLHMq10adSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1.) First training iteration\n",
        "\n",
        "The first manual training iteration is done with these hyperparameters:\n",
        "\n",
        "| Parameter | Value |\n",
        "|---|---|\n",
        "| Dense units | 480 |\n",
        "| Dropout rate | 0.5 |\n",
        "| L2 factor | 0.001 |\n",
        "| Optimizer | Adam |\n",
        "| Learning rate | 0.001 |"
      ],
      "metadata": {
        "id": "bXN689aj04ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = build_and_train(\n",
        "  dense_units = 480,\n",
        "  dropout_rate = 0.5,\n",
        "  l2_factor = 1e-3,\n",
        "  optimizer = 'adam',\n",
        "  learning_rate = 1e-3,\n",
        ")"
      ],
      "metadata": {
        "id": "uDxX_nUOM46h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(history)"
      ],
      "metadata": {
        "id": "TUGEuxlP3SBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(model)"
      ],
      "metadata": {
        "id": "Ue92w4hP4Kxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up some RAM for following trainings\n",
        "%reset_selective -f ^history$\n",
        "%reset_selective -f ^model$"
      ],
      "metadata": {
        "id": "_sioABZEeLe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2.) Second training iteration\n",
        "\n",
        "The second manual training iteration is done with these hyperparameters:\n",
        "\n",
        "| Parameter | Value |\n",
        "|---|---|\n",
        "| Dense units | 192 |\n",
        "| Dropout rate | 0.4 |\n",
        "| L2 factor | 0.0005 |\n",
        "| Optimizer | SGD |\n",
        "| Learning rate | 0.0001 |"
      ],
      "metadata": {
        "id": "90-jRMjz26dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = build_and_train(\n",
        "  dense_units = 192,\n",
        "  dropout_rate = 0.4,\n",
        "  l2_factor = 5e-4,\n",
        "  optimizer = 'sgd',\n",
        "  learning_rate = 1e-4,\n",
        ")"
      ],
      "metadata": {
        "id": "2tA4XZCWNEqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(history)"
      ],
      "metadata": {
        "id": "_XYo4-3-3ca6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(model)"
      ],
      "metadata": {
        "id": "C_5L7LWg4Twf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up some RAM for following trainings\n",
        "%reset_selective -f ^history$\n",
        "%reset_selective -f ^model$"
      ],
      "metadata": {
        "id": "P95jsDgLeR8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3.) Third training iteration\n",
        "\n",
        "The third manual training iteration is done with these hyperparameters:\n",
        "\n",
        "| Parameter | Value |\n",
        "|---|---|\n",
        "| Dense units | 96 |\n",
        "| Dropout rate | 0.2 |\n",
        "| L2 factor | 0.0002 |\n",
        "| Optimizer | Adam |\n",
        "| Learning rate | 0.0004 |"
      ],
      "metadata": {
        "id": "MdA2NGqW2-Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = build_and_train(\n",
        "  dense_units = 96,\n",
        "  dropout_rate = 0.2,\n",
        "  l2_factor = 2e-4,\n",
        "  optimizer = 'adam',\n",
        "  learning_rate = 4e-4,\n",
        ")"
      ],
      "metadata": {
        "id": "TCtswp0PUMs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(history)"
      ],
      "metadata": {
        "id": "CooMj-sl3c2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(model)"
      ],
      "metadata": {
        "id": "7wt0O5Oy4T8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up some RAM for following trainings\n",
        "%reset_selective -f ^history$\n",
        "%reset_selective -f ^model$"
      ],
      "metadata": {
        "id": "ZNzTgZPeeSa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.4.) Fourth training iteration\n",
        "\n",
        "The fourth manual training iteration is done with these hyperparameters:\n",
        "\n",
        "| Parameter | Value |\n",
        "|---|---|\n",
        "| Dense units | 32 |\n",
        "| Dropout rate | 0.1 |\n",
        "| L2 factor | 0.00002 |\n",
        "| Optimizer | RMSprop |\n",
        "| Learning rate | 0.0005 |"
      ],
      "metadata": {
        "id": "qPmNIx_33An-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = build_and_train(\n",
        "  dense_units = 32,\n",
        "  dropout_rate = 0.1,\n",
        "  l2_factor = 2e-5,\n",
        "  optimizer = 'rmsprop',\n",
        "  learning_rate = 5e-4,\n",
        ")"
      ],
      "metadata": {
        "id": "iEfZOoH8Uh07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(history)"
      ],
      "metadata": {
        "id": "LD4na8tA3dPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(model)"
      ],
      "metadata": {
        "id": "jQwcGa2e4UE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up some RAM for following trainings\n",
        "%reset_selective -f ^history$\n",
        "%reset_selective -f ^model$"
      ],
      "metadata": {
        "id": "uFymWaO7eSyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1MGR-a_fpSR"
      },
      "source": [
        "### 4.2.) Automatic training\n",
        "\n",
        "In order to find the best performing model, we use the *Keras Tuner* to perform a search, based on a custom hypermodel."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1.) Build hypermodel\n",
        "\n",
        "Our hypermodel is defined to build an ML model with the following range of hyperparameters:\n",
        "\n",
        "The second manual training iteration is done with these hyperparameters:\n",
        "\n",
        "| Parameter | Range |\n",
        "|---|---|\n",
        "| Dense units | 32 - 512 |\n",
        "| Dropout rate | 0.1 - 0.5 |\n",
        "| L2 factor | 0.00001 - 0.01 |\n",
        "| Optimizer | Adam, RMSprop, SGD |\n",
        "| Learning rate | 0.0001 - 0.01 |\n",
        "\n",
        "We use the **Bayesian optimization** tuner and perform a maximum of 10 trials. The best hyperparameters are calculated as a combination of the best **validation accuracy** and **validation loss** metrics."
      ],
      "metadata": {
        "id": "Jv2aJgi3b0Es"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdYGRVayfa1Q"
      },
      "outputs": [],
      "source": [
        "class BananaLeafDiseasesHyperModel(kt.HyperModel):\n",
        "  def __init__(self, name = None, tunable = True):\n",
        "    self.history = []\n",
        "\n",
        "    super().__init__(name, tunable)\n",
        "\n",
        "  def build(self, hp):\n",
        "    # Define layers\n",
        "    input_layer, outputs = define_layers(\n",
        "      dense_units = hp.Int('dense_units', min_value = 32, max_value = 512, step = 32),\n",
        "      dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value = 0.5, step = 0.1),\n",
        "      l2_factor = hp.Float('l2_factor', min_value = 1e-5, max_value = 1e-2, sampling = 'log'),\n",
        "    )\n",
        "    model = tf.keras.Model(inputs = input_layer, outputs = outputs)\n",
        "\n",
        "    # Compile model\n",
        "    compile_model(\n",
        "      model = model,\n",
        "      optimizer = hp.Choice('optimizer', values = ['adam', 'rmsprop', 'sgd']),\n",
        "      learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log'),\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "  def fit(self, hp, model, *args, **kwargs):\n",
        "    # Start training\n",
        "    history = super().fit(hp, model, *args, **kwargs)\n",
        "\n",
        "    # Track training results\n",
        "    self.history.append(history)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffRkq2n1lJ-H"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(\n",
        "  hypermodel = BananaLeafDiseasesHyperModel(),\n",
        "  objective = [\n",
        "    kt.Objective('val_accuracy', 'max'),\n",
        "    kt.Objective('val_loss', 'min'),\n",
        "  ],\n",
        "  max_trials = 10,\n",
        "  executions_per_trial = 1,\n",
        "  directory = os.path.join(training_dir, 'tuner'),\n",
        "  project_name = str(datetime.datetime.now().timestamp()),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2.) Search for best hyperparameters\n",
        "\n",
        "Similar to the manual training iterations, we now use the prebuilt tuner to start searching for the best model. We perform a maximum of 30 epochs per trial and use *Early Stopping* as defined earlier."
      ],
      "metadata": {
        "id": "OXChweSob636"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psY-BCWGlXlp"
      },
      "outputs": [],
      "source": [
        "tuner.search(\n",
        "  train_ds,\n",
        "  epochs = 30,\n",
        "  validation_data = val_ds,\n",
        "  callbacks = [create_early_stopping_callback()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFcSNbcrlYMZ"
      },
      "outputs": [],
      "source": [
        "# Fetch history from best trial\n",
        "best_trial = int(tuner.oracle.get_best_trials()[0].trial_id)\n",
        "best_history = tuner.hypermodel.history[best_trial].history\n",
        "\n",
        "# Fetch best performing model\n",
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqKANoUAHP6j"
      },
      "source": [
        "#### 4.2.3.) Evaluate best performing model\n",
        "\n",
        "All metrics of the best performing model are now visualized. In addition, we use the model to make predictions for the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(best_history)"
      ],
      "metadata": {
        "id": "7CXa59DqcPv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5iHcvzqos-h"
      },
      "outputs": [],
      "source": [
        "predict_and_evaluate(best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvv65DcuNcFw"
      },
      "source": [
        "## 5.) Save best performing model\n",
        "\n",
        "After we've found and trained our best performing model, we finally export it for later usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxdqsGt290uH"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(training_dir, 'model')\n",
        "best_model.save(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it. I hope you enjoyed it.\n",
        "\n",
        "ü§ñüëã"
      ],
      "metadata": {
        "id": "1bSVWknHQeT4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}